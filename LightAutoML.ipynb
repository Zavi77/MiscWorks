{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Tutorial_1. Create your own pipeline.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6VPKrH-qAgz"
      },
      "source": [
        "# Step 0.0. Install LightAutoML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hruCQVaPqAg0"
      },
      "source": [
        "Uncomment if doesn't clone repository by git. (ex.: colab, kaggle version)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c15ry11vqAg1",
        "outputId": "b4d3ece2-48a1-4c3e-98e6-fd35a98f7b51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! pip install -U lightautoml"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lightautoml\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/b7/eddea00dbc08237ba75d0bff3926def73e3be81afc3d2e9f4652c24fd1e8/LightAutoML-0.2.14-py3-none-any.whl (250kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 8.3MB/s \n",
            "\u001b[?25hCollecting importlib-metadata<2.0,>=1.0; python_version < \"3.8\"\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/58/cdea07eb51fc2b906db0968a94700866fc46249bdc75cac23f9d13168929/importlib_metadata-1.7.0-py2.py3-none-any.whl\n",
            "Collecting optuna\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/21/d13081805e1e1afc71f5bb743ece324c8bd576237c51b899ecb38a717502/optuna-2.7.0-py3-none-any.whl (293kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 48.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: networkx in /usr/local/lib/python3.7/dist-packages (from lightautoml) (2.5.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.7/dist-packages (from lightautoml) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from lightautoml) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: torchvision in /usr/local/lib/python3.7/dist-packages (from lightautoml) (0.9.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.7/dist-packages (from lightautoml) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from lightautoml) (2.11.3)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from lightautoml) (4.41.1)\n",
            "Collecting lightgbm<3.0,>=2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/9d/ddcb2f43aca194987f1a99e27edf41cf9bc39ea750c3371c2a62698c509a/lightgbm-2.3.1-py2.py3-none-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 22.7MB/s \n",
            "\u001b[?25hCollecting json2html\n",
            "  Downloading https://files.pythonhosted.org/packages/01/d5/40b617ee19d2d79f606ed37f8a81e51158f126d2af67270c68f2b47ae0d5/json2html-1.3.0.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: gensim in /usr/local/lib/python3.7/dist-packages (from lightautoml) (3.6.0)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.7/dist-packages (from lightautoml) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: webencodings in /usr/local/lib/python3.7/dist-packages (from lightautoml) (0.5.1)\n",
            "Collecting autowoe>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/5e/5be453ce2daa804b09d5b2cd9c2819f449a163cb227c8b8172479388f4f4/AutoWoE-1.2.5-py3-none-any.whl (204kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 49.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from lightautoml) (1.4.1)\n",
            "Collecting albumentations>=0.4.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/58/63fb1d742dc42d9ba2800ea741de1f2bc6bb05548d8724aa84794042eaf2/albumentations-0.5.2-py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 14.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from lightautoml) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from lightautoml) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from lightautoml) (0.22.2.post1)\n",
            "Collecting log-calls\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/e4/0163ac51514932824ae8962a2852ff46971aeed2e2a3dbd8717aaa8ace1a/log_calls-0.3.2.tar.gz (232kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 55.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from lightautoml) (1.8.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: seaborn in /usr/local/lib/python3.7/dist-packages (from lightautoml) (0.11.1)\n",
            "Requirement already satisfied, skipping upgrade: holidays in /usr/local/lib/python3.7/dist-packages (from lightautoml) (0.10.5.2)\n",
            "Collecting cmaes\n",
            "  Downloading https://files.pythonhosted.org/packages/01/1f/43b01223a0366171f474320c6e966c39a11587287f098a5f09809b45e05f/cmaes-0.8.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: pandas>=1 in /usr/local/lib/python3.7/dist-packages (from lightautoml) (1.1.5)\n",
            "Collecting efficientnet-pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/2e/a0/dd40b50aebf0028054b6b35062948da01123d7be38d08b6b1e5435df6363/efficientnet_pytorch-0.7.1.tar.gz\n",
            "Collecting catboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/80/8e9c57ec32dfed6ba2922bc5c96462cbf8596ce1a6f5de532ad1e43e53fe/catboost-0.25.1-cp37-none-manylinux1_x86_64.whl (67.3MB)\n",
            "\u001b[K     |████████████████████████████████| 67.3MB 40kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pywavelets in /usr/local/lib/python3.7/dist-packages (from lightautoml) (1.1.1)\n",
            "Collecting transformers>=4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 44.5MB/s \n",
            "\u001b[?25hCollecting poetry-core<2.0.0,>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/e1/08c7478df1e93dea47b06c9d9a80dbb54af7421462e1b22c280d063df807/poetry_core-1.0.3-py2.py3-none-any.whl (424kB)\n",
            "\u001b[K     |████████████████████████████████| 430kB 46.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<2.0,>=1.0; python_version < \"3.8\"->lightautoml) (3.4.1)\n",
            "Collecting cliff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/11/aea1cacbd4cf8262809c4d6f95dcb3f2802594de1f51c5bd454d69bf15c5/cliff-3.8.0-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.0MB/s \n",
            "\u001b[?25hCollecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/80/ef186e599a57d0e4cb78fc76e0bfc2e6953fa9716b2a5cf2de0117ed8eb5/alembic-1.6.5-py2.py3-none-any.whl (164kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 54.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna->lightautoml) (1.4.15)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna->lightautoml) (20.9)\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/32/e6/e9ddc6fa1104fda718338b341e4b3dc31cd8039ab29e52fc73b508515361/colorlog-5.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->lightautoml) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from nltk->lightautoml) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->lightautoml) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->lightautoml) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->lightautoml) (5.0.0)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->lightautoml) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->lightautoml) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: sphinx in /usr/local/lib/python3.7/dist-packages (from autowoe>=1.2->lightautoml) (1.8.5)\n",
            "Requirement already satisfied, skipping upgrade: pytest in /usr/local/lib/python3.7/dist-packages (from autowoe>=1.2->lightautoml) (3.6.4)\n",
            "Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.7/dist-packages (from autowoe>=1.2->lightautoml) (2018.9)\n",
            "Collecting sphinx-rtd-theme\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/24/2475e8f83519b54b2148d4a56eb1111f9cec630d088c3ffc214492c12107/sphinx_rtd_theme-0.5.2-py2.py3-none-any.whl (9.1MB)\n",
            "\u001b[K     |████████████████████████████████| 9.2MB 44.4MB/s \n",
            "\u001b[?25hCollecting imgaug>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl (948kB)\n",
            "\u001b[K     |████████████████████████████████| 952kB 48.7MB/s \n",
            "\u001b[?25hCollecting opencv-python-headless>=4.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/84/72ec52fbac4775c2a5bf0ee5573c922a0cac35eb841907edf56493a5e313/opencv_python_headless-4.5.2.52-cp37-cp37m-manylinux2014_x86_64.whl (38.2MB)\n",
            "\u001b[K     |████████████████████████████████| 38.2MB 64kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->lightautoml) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: korean-lunar-calendar in /usr/local/lib/python3.7/dist-packages (from holidays->lightautoml) (0.2.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil in /usr/local/lib/python3.7/dist-packages (from holidays->lightautoml) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: convertdate>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from holidays->lightautoml) (2.3.2)\n",
            "Requirement already satisfied, skipping upgrade: hijri-converter in /usr/local/lib/python3.7/dist-packages (from holidays->lightautoml) (2.1.1)\n",
            "Requirement already satisfied, skipping upgrade: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost->lightautoml) (0.10.1)\n",
            "Requirement already satisfied, skipping upgrade: plotly in /usr/local/lib/python3.7/dist-packages (from catboost->lightautoml) (4.4.1)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4->lightautoml) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=4->lightautoml) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 45.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4->lightautoml) (2019.12.20)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Collecting cmd2>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/8b/15061b32332bb35ea2a2f6263d0f616779d576e82739ec8e7fcf3c94abf5/cmd2-1.5.0-py3-none-any.whl (133kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 57.4MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/e0/1d4702dd81121d04a477c272d47ee5b6bc970d1a0990b11befa275c55cf2/pbr-5.6.0-py2.py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 54.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pyparsing>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->lightautoml) (2.4.7)\n",
            "Collecting stevedore>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/49/b602307aeac3df3384ff1fcd05da9c0376c622a6c48bb5325f28ab165b57/stevedore-3.3.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->lightautoml) (2.1.0)\n",
            "Collecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/54/dbc07fbb20865d3b78fdb7cf7fa713e2cba4f87f71100074ef2dc9f9d1f7/Mako-1.1.4-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 14.0MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: greenlet!=0.4.17; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna->lightautoml) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->lightautoml) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->lightautoml) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (1.2.4)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (56.1.0)\n",
            "Requirement already satisfied, skipping upgrade: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (0.7.12)\n",
            "Requirement already satisfied, skipping upgrade: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (2.9.1)\n",
            "Requirement already satisfied, skipping upgrade: docutils>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (0.17.1)\n",
            "Requirement already satisfied, skipping upgrade: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autowoe>=1.2->lightautoml) (8.7.0)\n",
            "Requirement already satisfied, skipping upgrade: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autowoe>=1.2->lightautoml) (1.4.0)\n",
            "Requirement already satisfied, skipping upgrade: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->autowoe>=1.2->lightautoml) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autowoe>=1.2->lightautoml) (1.10.0)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autowoe>=1.2->lightautoml) (21.2.0)\n",
            "Requirement already satisfied, skipping upgrade: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations>=0.4.6->lightautoml) (1.7.1)\n",
            "Requirement already satisfied, skipping upgrade: pymeeus<=1,>=0.3.13 in /usr/local/lib/python3.7/dist-packages (from convertdate>=2.3.0->holidays->lightautoml) (0.5.11)\n",
            "Requirement already satisfied, skipping upgrade: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost->lightautoml) (1.3.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4->lightautoml) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4->lightautoml) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4->lightautoml) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4->lightautoml) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4->lightautoml) (7.1.2)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a7/2c/4c64579f847bd5d539803c8b909e54ba087a79d01bb3aba433a95879a6c5/pyperclip-1.8.2.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->lightautoml) (0.2.5)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx->autowoe>=1.2->lightautoml) (1.1.4)\n",
            "Building wheels for collected packages: json2html, log-calls, efficientnet-pytorch, pyperclip\n",
            "  Building wheel for json2html (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for json2html: filename=json2html-1.3.0-cp37-none-any.whl size=7594 sha256=4b8859c4ec435ee8152f67866318de78aeb91bbdccf8e0e9f92735cdf060c40d\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/36/ad/386be30507bcb8f0c9830004bd776132eba63c1b945ed79255\n",
            "  Building wheel for log-calls (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for log-calls: filename=log_calls-0.3.2-cp37-none-any.whl size=51827 sha256=bf2e4efdafd3a9167154d1f19fadc58d8e55f2e9636f39c7547e7b3b588cdbcf\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/df/f7/db7e08f521e151e8d9abd7824845480b78a636d95237c601a4\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-cp37-none-any.whl size=16443 sha256=d4da68b0ff5292d450eddbd3f67e73032d60e0ea5ba55943be050a41c92ea262\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/27/aa/c46d23c4e8cc72d41283862b1437e0b3ad318417e8ed7d5921\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-cp37-none-any.whl size=11107 sha256=36301527aeae44a23ada403b53b516390d637a7b3fdb634a86f2b88837d73996\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/af/b8/3407109267803f4015e1ee2ff23be0c8c19ce4008665931ee1\n",
            "Successfully built json2html log-calls efficientnet-pytorch pyperclip\n",
            "\u001b[31mERROR: sphinx-rtd-theme 0.5.2 has requirement docutils<0.17, but you'll have docutils 0.17.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: importlib-metadata, pyperclip, colorama, cmd2, pbr, stevedore, cliff, Mako, python-editor, alembic, colorlog, cmaes, optuna, lightgbm, json2html, sphinx-rtd-theme, autowoe, imgaug, opencv-python-headless, albumentations, log-calls, efficientnet-pytorch, catboost, tokenizers, sacremoses, huggingface-hub, transformers, poetry-core, lightautoml\n",
            "  Found existing installation: importlib-metadata 4.0.1\n",
            "    Uninstalling importlib-metadata-4.0.1:\n",
            "      Successfully uninstalled importlib-metadata-4.0.1\n",
            "  Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed Mako-1.1.4 albumentations-0.5.2 alembic-1.6.5 autowoe-1.2.5 catboost-0.25.1 cliff-3.8.0 cmaes-0.8.2 cmd2-1.5.0 colorama-0.4.4 colorlog-5.0.1 efficientnet-pytorch-0.7.1 huggingface-hub-0.0.8 imgaug-0.4.0 importlib-metadata-1.7.0 json2html-1.3.0 lightautoml-0.2.14 lightgbm-2.3.1 log-calls-0.3.2 opencv-python-headless-4.5.2.52 optuna-2.7.0 pbr-5.6.0 poetry-core-1.0.3 pyperclip-1.8.2 python-editor-1.0.4 sacremoses-0.0.45 sphinx-rtd-theme-0.5.2 stevedore-3.3.0 tokenizers-0.10.3 transformers-4.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjGheIZjqAg3"
      },
      "source": [
        "# Step 0.1. Import necessary libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUCdcRLBqAg3",
        "outputId": "54423687-eb79-4252-80cf-b6ee524b4ccb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Standard python libraries\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "logging.basicConfig(format='[%(asctime)s] (%(levelname)s): %(message)s', level=logging.INFO)\n",
        "\n",
        "# Installed libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "# Imports from our package\n",
        "from lightautoml.automl.base import AutoML\n",
        "from lightautoml.ml_algo.boost_lgbm import BoostLGBM\n",
        "from lightautoml.ml_algo.tuning.optuna import OptunaTuner\n",
        "from lightautoml.pipelines.features.lgb_pipeline import LGBSimpleFeatures\n",
        "from lightautoml.pipelines.ml.base import MLPipeline\n",
        "from lightautoml.pipelines.selection.importance_based import ImportanceCutoffSelector, ModelBasedImportanceEstimator\n",
        "from lightautoml.reader.base import PandasToPandasReader\n",
        "from lightautoml.tasks import Task\n",
        "from lightautoml.utils.profiler import Profiler\n",
        "from lightautoml.automl.blend import WeightedBlender"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-05-30 12:50:28,405] (INFO): 'pattern' package not found; tag filters are not available for English\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdoTT2ihqAg5"
      },
      "source": [
        "# Step 0.2. Parameters "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koToIiobqAg5"
      },
      "source": [
        "N_THREADS = 8 # threads cnt for lgbm and linear models\n",
        "N_FOLDS = 5 # folds cnt for AutoML\n",
        "RANDOM_STATE = 42 # fixed random state for various reasons\n",
        "TEST_SIZE = 0.2 # Test size for metric check\n",
        "TARGET_NAME = 'Is_Lead' # Target column name"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4QJKqZeqAg6"
      },
      "source": [
        "# Step 0.3. Fix torch number of threads and numpy seed "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbZve86eqAg7"
      },
      "source": [
        "np.random.seed(RANDOM_STATE)\n",
        "torch.set_num_threads(N_THREADS)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZXCZ4D9qAg7"
      },
      "source": [
        "# Step 0.4. Change profiling decorators settings "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7yddlSqqAg8"
      },
      "source": [
        "By default, profiling decorators are turned off for speed and memory reduction. If you want to see profiling report after using LAMA, you need to turn on the decorators using command below: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKN_tTXxqAg8"
      },
      "source": [
        "p = Profiler()\n",
        "p.change_deco_settings({'enabled': True})"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLftWsWKq0dj",
        "outputId": "942799fa-e7ed-4124-899d-078dc07939bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX2SsBZiqAg9"
      },
      "source": [
        "# Step 0.5. Example data load "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36_zPoLPqAg9"
      },
      "source": [
        "Load a dataset from the repository if doesn't clone repository by git."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hJ9wCzrqAg-"
      },
      "source": [
        "DATASET_DIR = '/content/drive/MyDrive/'\n",
        "DATASET_NAME = 'train_new.csv'\n",
        "DATASET_FULLNAME = os.path.join(DATASET_DIR, DATASET_NAME)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmVb6sXZqAg-",
        "outputId": "0c031533-a219-4d84-977f-16a60fc185ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "if not os.path.exists(DATASET_FULLNAME):\n",
        "    os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "\n",
        "    dataset = requests.get(DATASET_URL).text\n",
        "    with open(DATASET_FULLNAME, 'w') as output:\n",
        "        output.write(dataset)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 0 ns, sys: 669 µs, total: 669 µs\n",
            "Wall time: 871 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlilZjizqAg_",
        "outputId": "9fa301c0-9d86-4278-ecc4-3171889ebd48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "data = pd.read_csv(DATASET_FULLNAME)\n",
        "data.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 150 ms, sys: 11.9 ms, total: 162 ms\n",
            "Wall time: 173 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkpFlkclqAhA"
      },
      "source": [
        "# Step 0.6. (Optional) Some user feature preparation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srsAlDCRqAhA"
      },
      "source": [
        "Cell below shows some user feature preparations to create task more difficult (this block can be omitted if you don't want to change the initial data):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7-jf4U90_rb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEtdbxhMwrYD",
        "outputId": "a2bbfc13-8574-414c-d85f-f08d93b574c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "# # import library\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# smote = SMOTE()\n",
        "\n",
        "# # fit predictor and target variable\n",
        "# x_smote, y_smote = smote.fit_resample(X, y)\n",
        "# X_train = pd.DataFrame(x_smote, columns=X.columns)\n",
        "# y_train = pd.DataFrame(y_smote, columns=['Is_Lead'])\n",
        "# data = pd.concat([X_train,y_train],axis=1)\n",
        "# data.head()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-05-30 10:45:42,405] (WARNING): /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning:\n",
            "\n",
            "Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Vintage</th>\n",
              "      <th>Avg_Account_Balance</th>\n",
              "      <th>Gender_Male</th>\n",
              "      <th>Region_Code_RG251</th>\n",
              "      <th>Region_Code_RG252</th>\n",
              "      <th>Region_Code_RG253</th>\n",
              "      <th>Region_Code_RG254</th>\n",
              "      <th>Region_Code_RG255</th>\n",
              "      <th>Region_Code_RG256</th>\n",
              "      <th>Region_Code_RG257</th>\n",
              "      <th>Region_Code_RG258</th>\n",
              "      <th>Region_Code_RG259</th>\n",
              "      <th>Region_Code_RG260</th>\n",
              "      <th>Region_Code_RG261</th>\n",
              "      <th>Region_Code_RG262</th>\n",
              "      <th>Region_Code_RG263</th>\n",
              "      <th>Region_Code_RG264</th>\n",
              "      <th>Region_Code_RG265</th>\n",
              "      <th>Region_Code_RG266</th>\n",
              "      <th>Region_Code_RG267</th>\n",
              "      <th>Region_Code_RG268</th>\n",
              "      <th>Region_Code_RG269</th>\n",
              "      <th>Region_Code_RG270</th>\n",
              "      <th>Region_Code_RG271</th>\n",
              "      <th>Region_Code_RG272</th>\n",
              "      <th>Region_Code_RG273</th>\n",
              "      <th>Region_Code_RG274</th>\n",
              "      <th>Region_Code_RG275</th>\n",
              "      <th>Region_Code_RG276</th>\n",
              "      <th>Region_Code_RG277</th>\n",
              "      <th>Region_Code_RG278</th>\n",
              "      <th>Region_Code_RG279</th>\n",
              "      <th>Region_Code_RG280</th>\n",
              "      <th>Region_Code_RG281</th>\n",
              "      <th>Region_Code_RG282</th>\n",
              "      <th>Region_Code_RG283</th>\n",
              "      <th>Region_Code_RG284</th>\n",
              "      <th>Occupation_Other</th>\n",
              "      <th>Occupation_Salaried</th>\n",
              "      <th>Occupation_Self_Employed</th>\n",
              "      <th>Channel_Code_X2</th>\n",
              "      <th>Channel_Code_X3</th>\n",
              "      <th>Channel_Code_X4</th>\n",
              "      <th>Credit_Product_Yes</th>\n",
              "      <th>Is_Active_Yes</th>\n",
              "      <th>Is_Lead</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>73</td>\n",
              "      <td>43</td>\n",
              "      <td>1045696</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30</td>\n",
              "      <td>32</td>\n",
              "      <td>581988</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56</td>\n",
              "      <td>26</td>\n",
              "      <td>1484315</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>34</td>\n",
              "      <td>19</td>\n",
              "      <td>470454</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>30</td>\n",
              "      <td>33</td>\n",
              "      <td>886787</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Age  Vintage  ...  Is_Active_Yes  Is_Lead\n",
              "0   73       43  ...              0      0.0\n",
              "1   30       32  ...              0      0.0\n",
              "2   56       26  ...              1      0.0\n",
              "3   34       19  ...              0      0.0\n",
              "4   30       33  ...              0      0.0\n",
              "\n",
              "[5 rows x 47 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qni76k1KqAhB"
      },
      "source": [
        "# Step 0.7. (Optional) Data splitting for train-test "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvR2BlpSqAhC"
      },
      "source": [
        "Block below can be omitted if you are going to train model only or you have specific train and test files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T77BR5VqAhC",
        "outputId": "ad291056-fa62-4871-97b4-6dc61c5a3c55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "train_data, test_data = train_test_split(data, \n",
        "                                         test_size=TEST_SIZE, \n",
        "                                         stratify=data[TARGET_NAME], \n",
        "                                         random_state=RANDOM_STATE)\n",
        "logging.info('Data splitted. Parts sizes: train_data = {}, test_data = {}'\n",
        "              .format(train_data.shape, test_data.shape))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-05-30 12:53:26,467] (INFO): Data splitted. Parts sizes: train_data = (196580, 10), test_data = (49145, 10)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 111 ms, sys: 398 µs, total: 112 ms\n",
            "Wall time: 114 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN96uDWgqAhD",
        "outputId": "d941365f-3d7e-43a1-f7b6-e9f702a89ea3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(train_data)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "196580"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv-ZlS7sqAhD"
      },
      "source": [
        "# ========= AutoML creation =========\n",
        "\n",
        "![AutoML pipeline for this task](https://github.com/sberbank-ai-lab/LightAutoML/blob/master/imgs/tutorial_1_pipeline.png?raw=1)\n",
        "\n",
        "\n",
        "## Step 1. Create Task and PandasReader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jbRwlyfqAhE",
        "outputId": "6e24decc-2da8-41ad-8115-97ae147d83eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "task = Task('binary')\n",
        "reader = PandasToPandasReader(task, cv=N_FOLDS, random_state=RANDOM_STATE)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6.27 ms, sys: 0 ns, total: 6.27 ms\n",
            "Wall time: 9.13 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrMMfiBUqAhE"
      },
      "source": [
        "## Step 2. Create feature selector (if necessary) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGb2MzH4qAhF",
        "outputId": "c3197b02-f019-4835-fef8-092e58b6be1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "model0 = BoostLGBM(\n",
        "    default_params={'learning_rate': 0.05, 'num_leaves': 64, 'seed': 42, 'num_threads': N_THREADS}\n",
        ")\n",
        "pipe0 = LGBSimpleFeatures()\n",
        "mbie = ModelBasedImportanceEstimator()\n",
        "selector = ImportanceCutoffSelector(pipe0, model0, mbie, cutoff=0)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.65 ms, sys: 99 µs, total: 4.75 ms\n",
            "Wall time: 7.22 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AzoChJrqAhG"
      },
      "source": [
        "## Step 3.1. Create 1st level ML pipeline for AutoML "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Bay5X5jqAhG"
      },
      "source": [
        "Our first level ML pipeline:\n",
        "- Simple features for gradient boosting built on selected features (using step 2) \n",
        "- 2 different models:\n",
        "    * LightGBM with params tuning (using OptunaTuner)\n",
        "    * LightGBM with heuristic params\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee7NM7zXqAhH",
        "outputId": "d1baa86a-5faf-441c-9f34-ed97931580a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time \n",
        "\n",
        "pipe = LGBSimpleFeatures()\n",
        "\n",
        "params_tuner1 = OptunaTuner(n_trials=20, timeout=30) # stop after 20 iterations or after 30 seconds \n",
        "model1 = BoostLGBM(\n",
        "    default_params={'learning_rate': 0.05, 'num_leaves': 128, 'seed': 1, 'num_threads': N_THREADS}\n",
        ")\n",
        "model2 = BoostLGBM(\n",
        "    default_params={'learning_rate': 0.025, 'num_leaves': 64, 'seed': 2, 'num_threads': N_THREADS}\n",
        ")\n",
        "\n",
        "pipeline_lvl1 = MLPipeline([\n",
        "    (model1, params_tuner1),\n",
        "    model2\n",
        "], pre_selection=selector, features_pipeline=pipe, post_selection=None)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3.11 ms, sys: 0 ns, total: 3.11 ms\n",
            "Wall time: 4.01 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "799R2PUPqAhI"
      },
      "source": [
        "## Step 3.2. Create 2nd level ML pipeline for AutoML "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQhk_nqmqAhI"
      },
      "source": [
        "Our second level ML pipeline:\n",
        "- Using simple features as well, but now it will be Out-Of-Fold (OOF) predictions of algos from 1st level\n",
        "- Only one LGBM model without params tuning\n",
        "- Without feature selection on this stage because we want to use all OOFs here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYqaCSxxqAhJ",
        "outputId": "f3efa991-c644-4db2-e0db-b5c085664669",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "pipe1 = LGBSimpleFeatures()\n",
        "\n",
        "model = BoostLGBM(\n",
        "    default_params={'learning_rate': 0.05, 'num_leaves': 64, 'max_bin': 1024, 'seed': 3, 'num_threads': N_THREADS},\n",
        "    freeze_defaults=True\n",
        ")\n",
        "\n",
        "pipeline_lvl2 = MLPipeline([model], pre_selection=None, features_pipeline=pipe1, post_selection=None)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.37 ms, sys: 0 ns, total: 1.37 ms\n",
            "Wall time: 1.38 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2Zaz5PLqAhJ"
      },
      "source": [
        "## Step 4. Create AutoML pipeline "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO6M8gcfqAhK"
      },
      "source": [
        "AutoML pipeline consist of:\n",
        "- Reader for data preparation\n",
        "- First level ML pipeline (as built in step 3.1)\n",
        "- Second level ML pipeline (as built in step 3.2)\n",
        "- `Skip_conn = False` equals here \"not to use initial features on the second level pipeline\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8vanP1oqAhK",
        "outputId": "3be55d45-4918-472d-f9d8-169959f55b99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time \n",
        "\n",
        "automl = AutoML(reader, [\n",
        "    [pipeline_lvl1],\n",
        "    [pipeline_lvl2],\n",
        "], skip_conn=False)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.25 ms, sys: 1.05 ms, total: 2.3 ms\n",
            "Wall time: 2.32 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxKtnkgSqAhL"
      },
      "source": [
        "## Step 5. Train AutoML on loaded data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTI6QVXLqAhL"
      },
      "source": [
        "In cell below we train AutoML with target column `TARGET` to receive fitted model and OOF predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVZykjyFqAhL",
        "outputId": "1685656e-e620-4712-9c98-82bf4bf20817",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time \n",
        "\n",
        "oof_pred = automl.fit_predict(train_data, roles={'target': TARGET_NAME})\n",
        "logging.info('oof_pred:\\n{}\\nShape = {}'.format(oof_pred, oof_pred.shape))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data shape: (196580, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[2021-05-30 12:54:06,584] (INFO): NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Feats was rejected during automatic roles guess: []\n",
            "\n",
            "\n",
            "Layer 1 ...\n",
            "Train process start. Time left 9999999982.866423 secs\n",
            "Start fitting LightGBM ...\n",
            "\n",
            "===== Start working with fold 0 for LightGBM =====\n",
            "\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's auc: 0.802499\n",
            "[200]\tvalid's auc: 0.801937\n",
            "Early stopping, best iteration is:\n",
            "[106]\tvalid's auc: 0.802588\n",
            "LightGBM fitting and predicting completed\n",
            "Optuna may run 6299999973.042978 secs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[2021-05-30 12:54:21,688] (INFO): A new study created in memory with name: no-name-71f05e08-8a26-4e70-81db-58fb32047881\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Start fitting Lvl_0_Pipe_0_Mod_0_LightGBM ...\n",
            "\n",
            "===== Start working with fold 0 for Lvl_0_Pipe_0_Mod_0_LightGBM =====\n",
            "\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's auc: 0.800743\n",
            "Early stopping, best iteration is:\n",
            "[64]\tvalid's auc: 0.80122\n",
            "Lvl_0_Pipe_0_Mod_0_LightGBM fitting and predicting completed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[2021-05-30 12:54:36,554] (INFO): Trial 0 finished with value: 0.8012195519172629 and parameters: {'feature_fraction': 0.6872700594236812, 'num_leaves': 244}. Best is trial 0 with value: 0.8012195519172629.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Start fitting Lvl_0_Pipe_0_Mod_0_LightGBM ...\n",
            "\n",
            "===== Start working with fold 0 for Lvl_0_Pipe_0_Mod_0_LightGBM =====\n",
            "\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's auc: 0.800836\n",
            "Early stopping, best iteration is:\n",
            "[60]\tvalid's auc: 0.801773\n",
            "Lvl_0_Pipe_0_Mod_0_LightGBM fitting and predicting completed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[2021-05-30 12:54:47,583] (INFO): Trial 1 finished with value: 0.8017731069410011 and parameters: {'feature_fraction': 0.8659969709057025, 'num_leaves': 159}. Best is trial 1 with value: 0.8017731069410011.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Start fitting Lvl_0_Pipe_0_Mod_0_LightGBM ...\n",
            "\n",
            "===== Start working with fold 0 for Lvl_0_Pipe_0_Mod_0_LightGBM =====\n",
            "\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's auc: 0.802143\n",
            "[200]\tvalid's auc: 0.80215\n",
            "Early stopping, best iteration is:\n",
            "[146]\tvalid's auc: 0.802433\n",
            "Lvl_0_Pipe_0_Mod_0_LightGBM fitting and predicting completed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[2021-05-30 12:54:56,725] (INFO): Trial 2 finished with value: 0.8024332472822987 and parameters: {'feature_fraction': 0.5780093202212182, 'num_leaves': 53}. Best is trial 2 with value: 0.8024332472822987.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Start fitting Lvl_0_Pipe_0_Mod_0_LightGBM ...\n",
            "\n",
            "===== Start working with fold 0 for Lvl_0_Pipe_0_Mod_0_LightGBM =====\n",
            "\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's auc: 0.802143\n",
            "[200]\tvalid's auc: 0.80215\n",
            "Early stopping, best iteration is:\n",
            "[146]\tvalid's auc: 0.802433\n",
            "\n",
            "===== Start working with fold 1 for Lvl_0_Pipe_0_Mod_0_LightGBM =====\n",
            "\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's auc: 0.797108\n",
            "[200]\tvalid's auc: 0.797217\n",
            "Early stopping, best iteration is:\n",
            "[183]\tvalid's auc: 0.797348\n",
            "\n",
            "===== Start working with fold 2 for Lvl_0_Pipe_0_Mod_0_LightGBM =====\n",
            "\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's auc: 0.801325\n",
            "[200]\tvalid's auc: 0.801099\n",
            "Early stopping, best iteration is:\n",
            "[114]\tvalid's auc: 0.801417\n",
            "\n",
            "===== Start working with fold 3 for Lvl_0_Pipe_0_Mod_0_LightGBM =====\n",
            "\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's auc: 0.79576\n",
            "[200]\tvalid's auc: 0.795712\n",
            "Early stopping, best iteration is:\n",
            "[127]\tvalid's auc: 0.795953\n",
            "\n",
            "===== Start working with fold 4 for Lvl_0_Pipe_0_Mod_0_LightGBM =====\n",
            "\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's auc: 0.801683\n",
            "[200]\tvalid's auc: 0.802126\n",
            "Early stopping, best iteration is:\n",
            "[169]\tvalid's auc: 0.802203\n",
            "Lvl_0_Pipe_0_Mod_0_LightGBM fitting and predicting completed\n",
            "Start fitting Lvl_0_Pipe_0_Mod_1_LightGBM ...\n",
            "\n",
            "===== Start working with fold 0 for Lvl_0_Pipe_0_Mod_1_LightGBM =====\n",
            "\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's auc: 0.801942\n",
            "[200]\tvalid's auc: 0.802483\n",
            "[300]\tvalid's auc: 0.802484\n",
            "Early stopping, best iteration is:\n",
            "[254]\tvalid's auc: 0.802585\n",
            "\n",
            "===== Start working with fold 1 for Lvl_0_Pipe_0_Mod_1_LightGBM =====\n",
            "\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's auc: 0.796532\n",
            "[200]\tvalid's auc: 0.797305\n",
            "[300]\tvalid's auc: 0.797509\n",
            "[400]\tvalid's auc: 0.7974\n",
            "Early stopping, best iteration is:\n",
            "[316]\tvalid's auc: 0.797522\n",
            "\n",
            "===== Start working with fold 2 for Lvl_0_Pipe_0_Mod_1_LightGBM =====\n",
            "\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's auc: 0.801367\n",
            "[200]\tvalid's auc: 0.801638\n",
            "[300]\tvalid's auc: 0.801693\n",
            "Early stopping, best iteration is:\n",
            "[230]\tvalid's auc: 0.801782\n",
            "\n",
            "===== Start working with fold 3 for Lvl_0_Pipe_0_Mod_1_LightGBM =====\n",
            "\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's auc: 0.795231\n",
            "[200]\tvalid's auc: 0.795964\n",
            "[300]\tvalid's auc: 0.795917\n",
            "Early stopping, best iteration is:\n",
            "[277]\tvalid's auc: 0.796019\n",
            "\n",
            "===== Start working with fold 4 for Lvl_0_Pipe_0_Mod_1_LightGBM =====\n",
            "\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's auc: 0.801092\n",
            "[200]\tvalid's auc: 0.802088\n",
            "[300]\tvalid's auc: 0.802256\n",
            "[400]\tvalid's auc: 0.802195\n",
            "[500]\tvalid's auc: 0.802113\n",
            "Early stopping, best iteration is:\n",
            "[428]\tvalid's auc: 0.802317\n",
            "Lvl_0_Pipe_0_Mod_1_LightGBM fitting and predicting completed\n",
            "Time left 9999999812.213236\n",
            "Layer 1 training completed.\n",
            "\n",
            "\n",
            "Layer 2 ...\n",
            "Train process start. Time left 9999999812.203081 secs\n",
            "Start fitting Lvl_1_Pipe_0_Mod_0_LightGBM ...\n",
            "\n",
            "===== Start working with fold 0 for Lvl_1_Pipe_0_Mod_0_LightGBM =====\n",
            "\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's auc: 0.801698\n",
            "Early stopping, best iteration is:\n",
            "[4]\tvalid's auc: 0.802206\n",
            "\n",
            "===== Start working with fold 1 for Lvl_1_Pipe_0_Mod_0_LightGBM =====\n",
            "\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's auc: 0.796239\n",
            "Early stopping, best iteration is:\n",
            "[32]\tvalid's auc: 0.796938\n",
            "\n",
            "===== Start working with fold 2 for Lvl_1_Pipe_0_Mod_0_LightGBM =====\n",
            "\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's auc: 0.800658\n",
            "Early stopping, best iteration is:\n",
            "[29]\tvalid's auc: 0.801195\n",
            "\n",
            "===== Start working with fold 3 for Lvl_1_Pipe_0_Mod_0_LightGBM =====\n",
            "\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's auc: 0.795236\n",
            "Early stopping, best iteration is:\n",
            "[21]\tvalid's auc: 0.795898\n",
            "\n",
            "===== Start working with fold 4 for Lvl_1_Pipe_0_Mod_0_LightGBM =====\n",
            "\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's auc: 0.800733\n",
            "Early stopping, best iteration is:\n",
            "[29]\tvalid's auc: 0.802035\n",
            "Lvl_1_Pipe_0_Mod_0_LightGBM fitting and predicting completed\n",
            "Time left 9999999788.076052\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[2021-05-30 12:57:27,689] (INFO): oof_pred:\n",
            "array([[0.2199113 ],\n",
            "       [0.20649932],\n",
            "       [0.30044973],\n",
            "       ...,\n",
            "       [0.23551995],\n",
            "       [0.11141734],\n",
            "       [0.09806565]], dtype=float32)\n",
            "Shape = (196580, 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5min 4s, sys: 51.5 s, total: 5min 55s\n",
            "Wall time: 3min 31s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v89By8AuqAhM"
      },
      "source": [
        "## Step 6. Analyze fitted model  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paOaETgVqAhM"
      },
      "source": [
        "Below we analyze feature importances of different algos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mF0iq5CqAhN"
      },
      "source": [
        "logging.info('Feature importances of selector:\\n{}'\n",
        "              .format(selector.get_features_score()))\n",
        "logging.info('=' * 70)\n",
        "\n",
        "logging.info('Feature importances of top level algorithm:\\n{}'\n",
        "              .format(automl.levels[-1][0].ml_algos[0].get_features_score()))\n",
        "logging.info('=' * 70)\n",
        "\n",
        "logging.info('Feature importances of lowest level algorithm - model 0:\\n{}'\n",
        "              .format(automl.levels[0][0].ml_algos[0].get_features_score()))\n",
        "logging.info('=' * 70)\n",
        "\n",
        "logging.info('Feature importances of lowest level algorithm - model 1:\\n{}'\n",
        "              .format(automl.levels[0][0].ml_algos[1].get_features_score()))\n",
        "logging.info('=' * 70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTDa22qDqAhN"
      },
      "source": [
        "## Step 7. Predict to test data and check scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_unWFUXqAhN",
        "outputId": "2ba8e7db-5760-4641-a05b-458d39056663",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "test_pred = automl.predict(test_data)\n",
        "logging.info('Prediction for test data:\\n{}\\nShape = {}'\n",
        "              .format(test_pred, test_pred.shape))\n",
        "\n",
        "logging.info('Check scores...')\n",
        "logging.info('OOF score: {}'.format(roc_auc_score(train_data[TARGET_NAME].values, oof_pred.data[:, 0])))\n",
        "logging.info('TEST score: {}'.format(roc_auc_score(test_data[TARGET_NAME].values, test_pred.data[:, 0])))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-05-30 13:01:22,962] (INFO): Prediction for test data:\n",
            "array([[0.15541112],\n",
            "       [0.14936374],\n",
            "       [0.7216174 ],\n",
            "       ...,\n",
            "       [0.1270724 ],\n",
            "       [0.11239512],\n",
            "       [0.2583373 ]], dtype=float32)\n",
            "Shape = (49145, 1)\n",
            "[2021-05-30 13:01:22,963] (INFO): Check scores...\n",
            "[2021-05-30 13:01:23,031] (INFO): OOF score: 0.7853082015972583\n",
            "[2021-05-30 13:01:23,049] (INFO): TEST score: 0.7991029890876837\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 10.3 s, sys: 4.42 ms, total: 10.3 s\n",
            "Wall time: 5.36 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9fW13VvtqON"
      },
      "source": [
        "\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "result = pd.DataFrame(test_df.ID,columns=['ID'])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqfTBWPisnu6"
      },
      "source": [
        "\n",
        "test_pred = automl.predict(test_df)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5ma41l2uHpL"
      },
      "source": [
        "\n",
        "result['Is_Lead'] = test_pred.data\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHdC9OChvN-L",
        "outputId": "f9d09522-68df-4212-e738-e3e1cd57c993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "result.head()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Is_Lead</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>VBENBARO</td>\n",
              "      <td>0.110636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CCMEWNKY</td>\n",
              "      <td>0.287544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>VK3KGA9M</td>\n",
              "      <td>0.131305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TT8RPZVC</td>\n",
              "      <td>0.104852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SHQZEYTZ</td>\n",
              "      <td>0.104746</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         ID   Is_Lead\n",
              "0  VBENBARO  0.110636\n",
              "1  CCMEWNKY  0.287544\n",
              "2  VK3KGA9M  0.131305\n",
              "3  TT8RPZVC  0.104852\n",
              "4  SHQZEYTZ  0.104746"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkdyWVUEuJbQ"
      },
      "source": [
        "result.to_csv('result11.csv',index=False)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Myfm12mqAhO"
      },
      "source": [
        "## Step 8. Profiling AutoML "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAGmjVEsqAhO"
      },
      "source": [
        "To build report here, we must turn on decorators on step 0.4. Report is interactive and you can go as deep into functions call stack as you want:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l-s0pjxqAhP"
      },
      "source": [
        "%%time\n",
        "p.profile('my_report_profile.html')\n",
        "assert os.path.exists('my_report_profile.html'), 'Profile report failed to build'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D2v9JyKqAhP"
      },
      "source": [
        "# Appendix. Profiling report screenshots "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N6Sn9gLqAhP"
      },
      "source": [
        "After loading HTML with profiling report, you can see fully folded report (please wait for green LOAD OK text for full load finish). If you click on triangle on the left, it unfolds and look like this:  \n",
        "\n",
        "<img src=\"https://github.com/sberbank-ai-lab/LightAutoML/blob/master/imgs/tutorial_1_initial_report.png?raw=1\" alt=\"Initial profiling report\" style=\"width: 500px;\"/>\n",
        "\n",
        "If we go even deeper we will receive situation like this:\n",
        "\n",
        "<img src=\"https://github.com/sberbank-ai-lab/LightAutoML/blob/master/imgs/tutorial_1_unfolded_report.png?raw=1\" alt=\"Profiling report after several unfoldings on different levels\" style=\"width: 500px;\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXL0QNTkqAhQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}